{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7872/410710250.py:21: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_7872/410710250.py:22: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "DEVELOPMENT_MODE = False\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
    "    %pip install circuitsvis\n",
    "    \n",
    "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
    "    # # Install another version of node that makes PySvelte work way faster\n",
    "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using renderer: colab\n"
     ]
    }
   ],
   "source": [
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-8ccc0ad7-3dc2\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-8ccc0ad7-3dc2\",\n",
       "      Hello,\n",
       "      {\"name\": \"Neel\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fbdc80451e0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "from functools import partial\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Neel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your device support faster inference by passing fp16=True in \"AutoModelForCausalLM.from_pretrained\".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969c69676e614f8aaa498fc701d1de1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import HookedEncoder\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "torch.set_grad_enabled(False)\n",
    "# Note: The default behavior now has injection attack prevention off.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat\", \n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat\", \n",
    "    device_map=\"cuda\", \n",
    "    trust_remote_code=True, \n",
    "    fp32=True,\n",
    "    use_logn_attn=False,\n",
    "    use_dynamic_ntk = False,\n",
    "    scale_attn_weights = False,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your device support faster inference by passing fp16=True in \"AutoModelForCausalLM.from_pretrained\".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7110143ec0704281bdba752f5cffb7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model qwen-1.8b-chat into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "qwen = HookedTransformer.from_pretrained(\n",
    "    'qwen-1.8b-chat', \n",
    "    fold_ln=False,\n",
    "    trust_remote_code=True,\n",
    "    center_writing_weights=False, \n",
    "    center_unembed=False,\n",
    "    device='cuda',\n",
    "    fp32=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "torch.float32 torch.float32\n",
      "torch.Size([1, 151936])\n",
      "torch.Size([1, 151936])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me how to make a bomb.\"\n",
    "template = f\"\"\"<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "def check_similarity_with_hf_model(tl_model, hf_model, atol, prompt=template):\n",
    "    tokens = tl_model.tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    logits = tl_model(tokens, prepend_bos=False).to(\"cuda\")[:, 0]\n",
    "    print(tokens.device)\n",
    "    hf_logits = hf_model(tokens).logits.to(\"cuda\")[:, 0]\n",
    "    print(logits.dtype, hf_logits.dtype)\n",
    "    print(logits.shape)\n",
    "    print(hf_logits.shape)\n",
    "    assert torch.allclose(logits, hf_logits, atol=atol)\n",
    "\n",
    "check_similarity_with_hf_model(qwen, model, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_rot_q', 'blocks.0.attn.hook_rot_k', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_pre_linear', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post'])\n",
      "dict_keys(['embed', '0.ln_1', '0.attn.c_attn', '0.attn.c_proj'])\n"
     ]
    }
   ],
   "source": [
    "tf_outputs = {}\n",
    "hf_outputs = {}\n",
    "LAYER = 0\n",
    "SEQ_POS = slice(None)\n",
    "def hook_fn(module, inputs, outputs, name):\n",
    "    hf_outputs[name] = outputs[:, SEQ_POS]\n",
    "\n",
    "hooks = []\n",
    "hooks.append(\n",
    "    model.transformer.wte.register_forward_hook(\n",
    "        partial(hook_fn, name='embed')\n",
    "    )\n",
    ")\n",
    "for name, module in model.transformer.h[LAYER].named_modules():\n",
    "    if len(name) > 0:\n",
    "        #print(f'Adding hook to {name}')\n",
    "        hooks.append(\n",
    "            module.register_forward_hook(\n",
    "                partial(hook_fn, name=f'{LAYER}.{name}')\n",
    "            )\n",
    "        )\n",
    "tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "try:\n",
    "    model(tokens)\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "def tl_fwd(act, hook):\n",
    "    #print(hook.name)\n",
    "    if hook.layer() == LAYER:\n",
    "        tf_outputs[hook.name] = act[:, SEQ_POS]\n",
    "\n",
    "def tf_hook(act, hook):\n",
    "    print(hook.name)\n",
    "    print(act[:, SEQ_POS])\n",
    "\n",
    "qwen.run_with_hooks(\n",
    "    tokens,\n",
    "    return_type=None,\n",
    "    fwd_hooks = [\n",
    "        (lambda name: name.startswith(f'blocks.{LAYER}'), tl_fwd)\n",
    "    ]\n",
    "    # fwd_hooks = [\n",
    "    #     (lambda name: name.startswith(f'blocks.{LAYER}.ln1'), tf_hook),\n",
    "    #     (lambda name: name.startswith(f'blocks.{LAYER}.hook_resid_post'), tf_hook)\n",
    "    # ]\n",
    ")\n",
    "\n",
    "print(tf_outputs.keys())\n",
    "print(hf_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks.0.hook_resid_pre False\n",
      "blocks.0.ln1.hook_normalized False\n",
      "blocks.0.hook_attn_out False\n",
      "blocks.0.hook_resid_mid False\n",
      "blocks.0.ln2.hook_normalized False\n",
      "blocks.0.hook_mlp_out False\n",
      "blocks.0.hook_resid_post False\n"
     ]
    }
   ],
   "source": [
    "MODULE = '0.attn.c_proj'\n",
    "for name, o in tf_outputs.items():\n",
    "    if o.shape == hf_outputs[MODULE].shape:\n",
    "        print(name, torch.allclose(o, hf_outputs[MODULE], atol=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0064, -0.0134, -0.0003,  ..., -0.0041,  0.0145,  0.0156],\n",
       "         [-0.0004, -0.0058,  0.0072,  ...,  0.0003,  0.0117,  0.0149],\n",
       "         [ 0.0022, -0.0093,  0.0108,  ..., -0.0023,  0.0013,  0.0164],\n",
       "         ...,\n",
       "         [ 0.0078,  0.0053,  0.0141,  ..., -0.0022,  0.0087, -0.0040],\n",
       "         [ 0.0076, -0.0001,  0.0088,  ..., -0.0004,  0.0067,  0.0034],\n",
       "         [ 0.0031, -0.0046,  0.0218,  ..., -0.0054,  0.0149, -0.0031]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_outputs['0.attn.c_proj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0064, -0.0134, -0.0003,  ..., -0.0041,  0.0145,  0.0156],\n",
       "         [-0.0069, -0.0059,  0.0137,  ...,  0.0014,  0.0006,  0.0145],\n",
       "         [ 0.0086, -0.0017,  0.0121,  ...,  0.0002,  0.0060,  0.0163],\n",
       "         ...,\n",
       "         [ 0.0184, -0.0019,  0.0308,  ..., -0.0104,  0.0099, -0.0037],\n",
       "         [ 0.0238, -0.0035,  0.0172,  ...,  0.0057, -0.0039,  0.0092],\n",
       "         [ 0.0030,  0.0071,  0.0153,  ..., -0.0176,  0.0193, -0.0182]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_outputs['blocks.0.hook_attn_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8272e-02, -5.0942e-03, -1.4467e-05,  ...,  8.2985e-03,\n",
      "           1.5530e-02, -4.3919e-02],\n",
      "         [ 3.6616e-02,  5.0892e-03,  2.0945e-04,  ...,  5.6465e-03,\n",
      "          -4.0636e-02,  3.2453e-03],\n",
      "         [ 8.0044e-03,  3.1383e-02, -4.8879e-05,  ...,  1.4352e-02,\n",
      "          -6.5369e-03,  5.5417e-02],\n",
      "         ...,\n",
      "         [-3.8649e-03, -3.7209e-02,  1.8876e-04,  ..., -1.9816e-02,\n",
      "           3.5827e-03, -1.2766e-02],\n",
      "         [-3.7146e-02, -1.1397e-02,  7.0360e-05,  ...,  1.6041e-02,\n",
      "          -1.5896e-02,  8.0627e-02],\n",
      "         [-6.7315e-02, -1.0729e-02,  4.8259e-04,  ..., -1.1246e-02,\n",
      "          -2.1827e-02,  6.7865e-02]]], device='cuda:0')\n",
      "tensor([[[ 9.7446e-03, -1.3926e-03,  3.0517e-08,  ...,  3.4793e-03,\n",
      "           5.8862e-03, -3.3735e-02],\n",
      "         [ 2.3355e-02,  1.6638e-03, -5.2841e-07,  ...,  2.8314e-03,\n",
      "          -1.8420e-02,  2.9813e-03],\n",
      "         [ 4.3132e-03,  8.6682e-03,  1.0418e-07,  ...,  6.0798e-03,\n",
      "          -2.5034e-03,  4.3008e-02],\n",
      "         ...,\n",
      "         [-2.3215e-03, -1.1456e-02, -4.4847e-07,  ..., -9.3574e-03,\n",
      "           1.5294e-03, -1.1044e-02],\n",
      "         [-1.8032e-02, -2.8360e-03, -1.3510e-07,  ...,  6.1219e-03,\n",
      "          -5.4840e-03,  5.6371e-02],\n",
      "         [-4.2182e-02, -3.4460e-03, -1.1961e-06,  ..., -5.5400e-03,\n",
      "          -9.7205e-03,  6.1249e-02]]], device='cuda:0')\n",
      "tensor([[[ 6.5157e-04, -4.7729e-05, -8.0710e-12,  ...,  1.8290e-04,\n",
      "           2.7972e-04, -3.2487e-03],\n",
      "         [ 1.9651e-03,  7.1761e-05,  1.7586e-10,  ...,  1.8729e-04,\n",
      "          -1.1015e-03,  3.6129e-04],\n",
      "         [ 3.3078e-04,  3.4075e-04, -3.1602e-11,  ...,  3.6656e-04,\n",
      "          -1.3644e-04,  4.7504e-03],\n",
      "         ...,\n",
      "         [-2.0032e-04, -5.0670e-04,  1.5306e-10,  ..., -6.3477e-04,\n",
      "           9.3789e-05, -1.3725e-03],\n",
      "         [-1.1713e-03, -9.4420e-05,  3.4709e-11,  ...,  3.1260e-04,\n",
      "          -2.5315e-04,  5.2734e-03],\n",
      "         [-3.4849e-03, -1.4593e-04,  3.9087e-10,  ..., -3.5982e-04,\n",
      "          -5.7074e-04,  7.2879e-03]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = tf_outputs['blocks.0.hook_resid_pre']\n",
    "\n",
    "scale = (x.pow(2).mean(dim=-1, keepdim=True) + 1e-6).sqrt()\n",
    "x /= scale\n",
    "x *= qwen.blocks[0].ln1.w\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
